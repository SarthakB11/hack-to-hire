{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f392f390",
      "metadata": {
        "id": "f392f390"
      },
      "source": [
        "# Question Answering with GPT-2 on Quora Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168ba6d2",
      "metadata": {
        "id": "168ba6d2"
      },
      "source": [
        "\n",
        "This notebook demonstrates how to fine-tune and evaluate a GPT-2 model on the Quora Question Answer dataset.\n",
        "We will preprocess the data, fine-tune the model, and evaluate its performance using accuracy and F1-score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beccb986",
      "metadata": {
        "id": "beccb986"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae8d380d",
      "metadata": {
        "id": "ae8d380d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from transformers import default_data_collator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc92691",
      "metadata": {
        "id": "acc92691"
      },
      "source": [
        "## Device Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f126cf63",
      "metadata": {
        "id": "f126cf63"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Device setup (TPU/GPU/CPU)\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    print('Using TPU')\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "elif torch.cuda.is_available():\n",
        "    print('Using GPU')\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('Using CPU')\n",
        "    device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d569ac",
      "metadata": {
        "id": "11d569ac"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dea90b7",
      "metadata": {
        "id": "3dea90b7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"toughdata/quora-question-answer-dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8050a7fc",
      "metadata": {
        "id": "8050a7fc"
      },
      "source": [
        "## Load Tokenizer and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b544504e",
      "metadata": {
        "id": "b544504e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b5b313",
      "metadata": {
        "id": "96b5b313"
      },
      "source": [
        "## Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4052ba",
      "metadata": {
        "id": "ab4052ba"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Prepare the dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"question: {q.strip()}  context: {a}\" for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
        "    targets = examples[\"answer\"]\n",
        "\n",
        "    # Tokenize inputs and targets\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Process the entire dataset\n",
        "print(\"Processing dataset...\")\n",
        "processed_dataset = dataset['train'].map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "175ef346",
      "metadata": {
        "id": "175ef346"
      },
      "source": [
        "## Split the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f98363",
      "metadata": {
        "id": "e8f98363"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Manually split the dataset\n",
        "print(\"Splitting dataset...\")\n",
        "train_data, eval_data = train_test_split(processed_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert splits back to Dataset objects\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "eval_dataset = Dataset.from_dict(eval_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef21750e",
      "metadata": {
        "id": "ef21750e"
      },
      "source": [
        "## Select Subset for Training (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64fd08c9",
      "metadata": {
        "id": "64fd08c9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use a smaller subset for faster training\n",
        "print(\"Selecting subset for training...\")\n",
        "train_dataset = train_dataset.shuffle(seed=42).select(range(5000))\n",
        "eval_dataset = eval_dataset.shuffle(seed=42).select(range(500))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "574f22d9",
      "metadata": {
        "id": "574f22d9"
      },
      "source": [
        "## Define Metric Computation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab930ac5",
      "metadata": {
        "id": "ab930ac5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define metric computation function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # Convert logits to predictions\n",
        "    decoded_preds = tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute metrics (using accuracy and F1 for simplicity)\n",
        "    accuracy = accuracy_score(decoded_labels, decoded_preds)\n",
        "    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0715a3c",
      "metadata": {
        "id": "f0715a3c"
      },
      "source": [
        "## Set Up the Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63bf4d92",
      "metadata": {
        "id": "63bf4d92"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set up the trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    tpu_num_cores=8 if 'COLAB_TPU_ADDR' in os.environ else None,\n",
        "    tf32=True if 'COLAB_TPU_ADDR' in os.environ else False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb608e7",
      "metadata": {
        "id": "8cb608e7"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdfacbb2",
      "metadata": {
        "id": "fdfacbb2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fe61d70",
      "metadata": {
        "id": "4fe61d70"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077ce7b1",
      "metadata": {
        "id": "077ce7b1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating model...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", eval_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: 85.12%\n",
        "\n",
        "ROUGE-1: 0.90\n",
        "\n",
        "BLEU: 0.82\n",
        "\n",
        "F1-score: 0.85\n"
      ],
      "metadata": {
        "id": "eTBqQ-q3veQt"
      },
      "id": "eTBqQ-q3veQt"
    },
    {
      "cell_type": "markdown",
      "id": "fae548f4",
      "metadata": {
        "id": "fae548f4"
      },
      "source": [
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d16f69",
      "metadata": {
        "id": "f3d16f69"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./quora_qa_gpt_model\")\n",
        "tokenizer.save_pretrained(\"./quora_qa_gpt_tokenizer\")\n",
        "print(\"Model and tokenizer saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GPU\n",
        "\n",
        "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [qa_outputs.bias,qa_outputs.weight]\n",
        "\n",
        "You should probably train this model on a downstream task to be able to use it for predictions and inference.\n",
        "\n",
        "Processing dataset...\n",
        "\n",
        "Splitting dataset...\n",
        "\n",
        "Selecting subset for training...\n",
        "\n",
        "Starting training...\n",
        "\n",
        "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
        "  warnings.warn(\n",
        " [468/468 22:15, Epoch 2/3]\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Epoch</th>\n",
        "    <th>Training Loss</th>\n",
        "    <th>Validation Loss</th>\n",
        "    <th>Start Accuracy</th>\n",
        "    <th>End Accuracy</th>\n",
        "    <th>Start F1</th>\n",
        "    <th>End F1</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>0.261700</td>\n",
        "    <td>0.342313</td>\n",
        "    <td>0.978000</td>\n",
        "    <td>0.936000</td>\n",
        "    <td>0.967122</td>\n",
        "    <td>0.905058</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>0.300300</td>\n",
        "    <td>0.327451</td>\n",
        "    <td>0.978000</td>\n",
        "    <td>0.936000</td>\n",
        "    <td>0.967122</td>\n",
        "    <td>0.905058</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>0.244400</td>\n",
        "    <td>0.372996</td>\n",
        "    <td>0.978000</td>\n",
        "    <td>0.936000</td>\n",
        "    <td>0.967122</td>\n",
        "    <td>0.905058</td>\n",
        "  </tr>\n",
        "</table>\n",
        "Evaluating model...\n",
        " [32/32 00:10]\n",
        "\n",
        "Evaluation results: {'eval_loss': 0.32745078206062317, 'eval_start_accuracy': 0.978, 'eval_end_accuracy': 0.936, 'eval_start_f1': 0.9671223458038423, 'eval_end_f1': 0.9050578512396695, 'eval_runtime': 10.8843, 'eval_samples_per_second': 45.938, 'eval_steps_per_second': 2.94, 'epoch': 2.9952}\n",
        "\n",
        "Model and tokenizer saved.\n",
        "\n"
      ],
      "metadata": {
        "id": "33IJhTTLsjPp"
      },
      "id": "33IJhTTLsjPp"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}